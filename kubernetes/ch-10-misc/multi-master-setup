MULTI-MASTER SETUP

Cloud Platform used - AWS
Instances created - 
2 machines for master, ubuntu 16.04+, 2 CPU, 2 GB RAM, 10 GB storage
2 machines for worker, ubuntu 16.04+, 1 CPU, 2 GB RAM, 10 GB storage
1 machine for loadbalancer, ubuntu 16.04+, 1 CPU, 2 GB RAM, 10 GB storage

<Note down the Internal IP addresses of all machines>
<Change the hostnames of managers to manager1 and manager2 and workers to worker1 and worker2>
<Recommended to run "top" after you do SSH into the machines, so connection won't drop>
<Also make sure to add entry of IP to hostname in /etc/hosts in ALL machines>


#####
STEP #1 - Configuring Load Balancer
#####

1. SSH into the loadbalancer machine 
2. Switch to root using sudo -i
3. Change hostname (recommended), repeat the same step for all machines just change names to manager1/manager2/worker1/worker2
          # hostnamectl set-hostname loadbalancer
          # vim /etc/hosts
               127.0.0.1 localhost loadbalancer
	       x.x.x.x loadbalancer
4. Reboot to check
5. Update repo and system
          # sudo apt-get update && sudo apt-get upgrade -y
6. Install HAProxy
	  # sudo apt-get install haproxy -y
7. Edit haproxy configuration file
          # vim /etc/haproxy/haproxy.cfg

   --- add these lines at bottom of file <respect the indents>
       frontend fe-apiserver
           	bind 0.0.0.0:6443
           	mode tcp
           	option tcplog
           	default_backend be-apiserver
  
       backend be-apiserver
   		mode tcp
   		option tcplog
   		option tcp-check
   		balance roundrobin
   		default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
       		server manager1 <manager1-ip-address>:6443 check
       		server manager2 <manager2-ip-address>:6443 check

    --- save and exit
8. Restart haproxy and check
	# systemctl restart haproxy
	# systemctl enable haproxy
	# systemctl status haproxy
	# nc -v localhost 6443


#####
STEP #2
#####

Installing kubeadm / kubelet and docker on manager1/manager2/worker1/worker2

Create a file for-all-machines.sh, with these contents and run on all machines except loadbalancer

########### for-all-machines.sh ########
#!/bin/bash
echo "disabling swap"
swapoff -a
sed -e '/swap/s/^/#/g' -i /etc/fstab
echo "installing kubernetes version 1.24.1-00"
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y kubelet=1.24.1-00 kubeadm=1.24.1-00 kubectl=1.24.1-00 docker.io
apt-mark hold kubelet kubeadm kubectl
cat <<EOF | sudo tee /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
systemctl daemon-reload
systemctl restart docker
systemctl restart kubelet

 
echo "ALL DONE - OK"

####### File for-all-machines.sh ENDS here


#####
STEP 3
#####

Initialize manager1 to bootstrap cluster

1. SSH into manager1
2. Get root access using sudo -i
3. Execute the following command, just replace your loadbalancer internal IP

	# kubeadm init --control-plane-endpoint "<loadbalancer-internal-IP>:6443" --upload-certs --pod-network-cidr=192.168.0.0/16

4. Copy the output of SUCCESS in a file.

#####
STEP 4
#####

Login to manager2

1. SSH into manager2
2. Get root access using sudo -i
3. Paste the token of manager copied from STEP 3. Something like this:

	kubeadm join loadbalancer:6443 --token cnslau.kd5fjt96jeuzymzb \
    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 \
    --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146
 
#####
STEP 5
#####

Login in worker1 and worker2 and repeat these steps

1. SSH into worker1/worker2
2. Get root access using sudo -i
3. Paste the token of joining worker copied from STEP 3. Something like this:
	kubeadm join loadbalancer:6443 --token cnslau.kd5fjt96jeuzymzb \
    --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 

#####
STEP 6
#####

Finally configure loadbalancer with kubeconfig and install kubectl 

1. SSH into loadbalancer
2. Get root access using sudo -i
3. Create a directory - mkdir -p $HOME/.kube
4. Copy the contents of /etc/kubernetes/admin.conf from master1 in a file named "config" inside $HOME/.kube/
5. Install kubectl 
	# snap install kubectl --classic
6. Check cluster
	# kubectl get nodes
	<Nodes will not be ready>
7. Install Overlay network/CNI
	# kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
	# curl https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml -O
	# kubectl create -f custom-resources.yaml

8. Again check the cluster, nodes will be READY, it may take some time
	# kubectl get nodes

	

 
